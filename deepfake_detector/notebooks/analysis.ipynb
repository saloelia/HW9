{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepFake Detection Analysis Notebook\n",
        "\n",
        "This notebook provides comprehensive analysis and experimentation for the DeepFake Detection system.\n",
        "\n",
        "## Contents\n",
        "1. Setup and Configuration\n",
        "2. Load Batch Experiment Results\n",
        "3. Detection Performance Analysis\n",
        "4. Comparison: Real vs Fake Videos\n",
        "5. Confidence Distribution Analysis\n",
        "6. Detailed Results Visualization\n",
        "7. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Batch Experiment Results\n",
        "\n",
        "This section loads results from batch experiments run with `python demo.py --batch`.\n",
        "\n",
        "Results are stored in `results/batch_experiment_<timestamp>.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_latest_experiment(results_dir: Path = None) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Load the most recent batch experiment results.\n",
        "    \"\"\"\n",
        "    if results_dir is None:\n",
        "        results_dir = project_root / \"results\"\n",
        "    \n",
        "    # Find all batch experiment files\n",
        "    experiment_files = sorted(\n",
        "        glob(str(results_dir / \"batch_experiment_*.json\")),\n",
        "        reverse=True  # Most recent first\n",
        "    )\n",
        "    \n",
        "    if not experiment_files:\n",
        "        print(\"No batch experiment results found!\")\n",
        "        print(\"Run: python demo.py --batch\")\n",
        "        return None\n",
        "    \n",
        "    latest_file = experiment_files[0]\n",
        "    print(f\"Loading: {Path(latest_file).name}\")\n",
        "    \n",
        "    with open(latest_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def load_all_experiments(results_dir: Path = None) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load all batch experiment results.\n",
        "    \"\"\"\n",
        "    if results_dir is None:\n",
        "        results_dir = project_root / \"results\"\n",
        "    \n",
        "    experiment_files = sorted(\n",
        "        glob(str(results_dir / \"batch_experiment_*.json\")),\n",
        "        reverse=True\n",
        "    )\n",
        "    \n",
        "    all_experiments = []\n",
        "    for f in experiment_files:\n",
        "        with open(f, 'r') as file:\n",
        "            all_experiments.append(json.load(file))\n",
        "    \n",
        "    return all_experiments\n",
        "\n",
        "\n",
        "# Load the latest experiment\n",
        "experiment_data = load_latest_experiment()\n",
        "\n",
        "if experiment_data:\n",
        "    print(f\"\\nExperiment Timestamp: {experiment_data['timestamp']}\")\n",
        "    print(f\"LLM Provider: {experiment_data['llm_provider']}\")\n",
        "    print(f\"Model: {experiment_data['llm_model']}\")\n",
        "    print(f\"\\nSummary:\")\n",
        "    for key, value in experiment_data['summary'].items():\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract results for analysis\n",
        "if experiment_data:\n",
        "    results = experiment_data['results']\n",
        "    \n",
        "    # Separate by ground truth\n",
        "    real_results = [r for r in results if r['ground_truth'] == 'REAL']\n",
        "    fake_results = [r for r in results if r['ground_truth'] == 'FAKE']\n",
        "    \n",
        "    print(f\"Total videos analyzed: {len(results)}\")\n",
        "    print(f\"  - Real videos: {len(real_results)}\")\n",
        "    print(f\"  - Fake videos: {len(fake_results)}\")\n",
        "    \n",
        "    # Display detailed results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for r in results:\n",
        "        status = \"CORRECT\" if r.get('correct', False) else \"INCORRECT\"\n",
        "        print(f\"\\n{r['video']}\")\n",
        "        print(f\"  Ground Truth: {r['ground_truth']}\")\n",
        "        print(f\"  Predicted: {r['predicted']}\")\n",
        "        print(f\"  Confidence: {r['confidence']:.1%}\")\n",
        "        print(f\"  Status: {status}\")\n",
        "else:\n",
        "    results = []\n",
        "    real_results = []\n",
        "    fake_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Detection Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(results: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate detection performance metrics.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return {}\n",
        "    \n",
        "    # True Positives: Fake videos correctly identified as fake\n",
        "    # True Negatives: Real videos correctly identified as real\n",
        "    # False Positives: Real videos incorrectly identified as fake\n",
        "    # False Negatives: Fake videos incorrectly identified as real\n",
        "    \n",
        "    tp = sum(1 for r in results if r['ground_truth'] == 'FAKE' and r['predicted'] == 'FAKE')\n",
        "    tn = sum(1 for r in results if r['ground_truth'] == 'REAL' and r['predicted'] == 'REAL')\n",
        "    fp = sum(1 for r in results if r['ground_truth'] == 'REAL' and r['predicted'] == 'FAKE')\n",
        "    fn = sum(1 for r in results if r['ground_truth'] == 'FAKE' and r['predicted'] == 'REAL')\n",
        "    \n",
        "    # Handle UNCERTAIN predictions\n",
        "    uncertain = sum(1 for r in results if r['predicted'] == 'UNCERTAIN')\n",
        "    \n",
        "    total = len(results)\n",
        "    correct = tp + tn\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'true_positives': tp,\n",
        "        'true_negatives': tn,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn,\n",
        "        'uncertain': uncertain,\n",
        "        'total': total,\n",
        "    }\n",
        "\n",
        "\n",
        "if results:\n",
        "    metrics = calculate_metrics(results)\n",
        "    \n",
        "    print(\"DETECTION PERFORMANCE METRICS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\\nAccuracy:  {metrics['accuracy']:.1%}\")\n",
        "    print(f\"Precision: {metrics['precision']:.1%}\")\n",
        "    print(f\"Recall:    {metrics['recall']:.1%}\")\n",
        "    print(f\"F1 Score:  {metrics['f1_score']:.3f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"  True Positives (Fake detected as Fake): {metrics['true_positives']}\")\n",
        "    print(f\"  True Negatives (Real detected as Real): {metrics['true_negatives']}\")\n",
        "    print(f\"  False Positives (Real detected as Fake): {metrics['false_positives']}\")\n",
        "    print(f\"  False Negatives (Fake detected as Real): {metrics['false_negatives']}\")\n",
        "    if metrics['uncertain'] > 0:\n",
        "        print(f\"  Uncertain: {metrics['uncertain']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Confusion Matrix\n",
        "if results and len(results) > 0:\n",
        "    metrics = calculate_metrics(results)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    cm = np.array([\n",
        "        [metrics['true_negatives'], metrics['false_positives']],\n",
        "        [metrics['false_negatives'], metrics['true_positives']]\n",
        "    ])\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Predicted Real', 'Predicted Fake'],\n",
        "                yticklabels=['Actual Real', 'Actual Fake'],\n",
        "                annot_kws={'size': 16})\n",
        "    \n",
        "    ax.set_title(f'Confusion Matrix\\nAccuracy: {metrics[\"accuracy\"]:.1%}', fontsize=14)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "    ax.set_ylabel('True Label', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\nConfusion matrix saved to 'confusion_matrix.png'\")\n",
        "else:\n",
        "    print(\"No results to visualize. Run batch experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison: Real vs Fake Videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare confidence scores between real and fake videos\n",
        "if results and len(results) > 0:\n",
        "    real_confidences = [r['confidence'] for r in real_results if r.get('confidence')]\n",
        "    fake_confidences = [r['confidence'] for r in fake_results if r.get('confidence')]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Confidence Distribution\n",
        "    ax1 = axes[0]\n",
        "    if real_confidences:\n",
        "        ax1.hist(real_confidences, bins=10, alpha=0.6, label=f'Real Videos (n={len(real_confidences)})', \n",
        "                color='green', edgecolor='darkgreen')\n",
        "    if fake_confidences:\n",
        "        ax1.hist(fake_confidences, bins=10, alpha=0.6, label=f'Fake Videos (n={len(fake_confidences)})', \n",
        "                color='red', edgecolor='darkred')\n",
        "    ax1.axvline(x=0.7, color='black', linestyle='--', label='Threshold (0.7)')\n",
        "    ax1.set_xlabel('Confidence Score', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.set_title('Confidence Score Distribution', fontsize=14)\n",
        "    ax1.legend()\n",
        "    ax1.set_xlim(0, 1)\n",
        "    \n",
        "    # Box Plot\n",
        "    ax2 = axes[1]\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "    colors = []\n",
        "    \n",
        "    if real_confidences:\n",
        "        data_to_plot.append(real_confidences)\n",
        "        labels.append(f'Real\\n(n={len(real_confidences)})')\n",
        "        colors.append('green')\n",
        "    if fake_confidences:\n",
        "        data_to_plot.append(fake_confidences)\n",
        "        labels.append(f'Fake\\n(n={len(fake_confidences)})')\n",
        "        colors.append('red')\n",
        "    \n",
        "    if data_to_plot:\n",
        "        bp = ax2.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
        "        for patch, color in zip(bp['boxes'], colors):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_alpha(0.5)\n",
        "    \n",
        "    ax2.axhline(y=0.7, color='black', linestyle='--', alpha=0.5, label='Threshold')\n",
        "    ax2.set_ylabel('Confidence Score', fontsize=12)\n",
        "    ax2.set_title('Confidence by Video Type', fontsize=14)\n",
        "    ax2.set_ylim(0, 1.05)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confidence_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\nConfidence comparison saved to 'confidence_comparison.png'\")\n",
        "else:\n",
        "    print(\"No results to visualize. Run batch experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Per-Video Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed per-video analysis\n",
        "if results and len(results) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, max(6, len(results) * 0.5)))\n",
        "    \n",
        "    videos = [r['video'] for r in results]\n",
        "    confidences = [r['confidence'] for r in results]\n",
        "    colors = ['green' if r['correct'] else 'red' for r in results]\n",
        "    ground_truths = [r['ground_truth'] for r in results]\n",
        "    \n",
        "    y_pos = np.arange(len(videos))\n",
        "    \n",
        "    bars = ax.barh(y_pos, confidences, color=colors, alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    # Add labels\n",
        "    for i, (bar, gt, conf, correct) in enumerate(zip(bars, ground_truths, confidences, [r['correct'] for r in results])):\n",
        "        status = \"Correct\" if correct else \"Incorrect\"\n",
        "        ax.text(conf + 0.02, i, f'{gt} - {status}', va='center', fontsize=10)\n",
        "    \n",
        "    ax.axvline(x=0.7, color='orange', linestyle='--', linewidth=2, label='Threshold (0.7)')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(videos)\n",
        "    ax.set_xlabel('Confidence Score', fontsize=12)\n",
        "    ax.set_title('Detection Results by Video\\n(Green = Correct, Red = Incorrect)', fontsize=14)\n",
        "    ax.set_xlim(0, 1.2)\n",
        "    ax.legend(loc='upper right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('per_video_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\nPer-video analysis saved to 'per_video_analysis.png'\")\n",
        "else:\n",
        "    print(\"No results to visualize. Run batch experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Processing Time Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze processing times\n",
        "if results and len(results) > 0:\n",
        "    processing_times = [r.get('processing_time', 0) for r in results if r.get('processing_time')]\n",
        "    \n",
        "    if processing_times:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Histogram\n",
        "        ax1 = axes[0]\n",
        "        ax1.hist(processing_times, bins=10, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "        ax1.axvline(x=np.mean(processing_times), color='red', linestyle='--', \n",
        "                   label=f'Mean: {np.mean(processing_times):.1f}s')\n",
        "        ax1.set_xlabel('Processing Time (seconds)', fontsize=12)\n",
        "        ax1.set_ylabel('Count', fontsize=12)\n",
        "        ax1.set_title('Processing Time Distribution', fontsize=14)\n",
        "        ax1.legend()\n",
        "        \n",
        "        # Per-video bar chart\n",
        "        ax2 = axes[1]\n",
        "        videos_with_time = [(r['video'], r.get('processing_time', 0)) for r in results if r.get('processing_time')]\n",
        "        videos, times = zip(*videos_with_time)\n",
        "        y_pos = np.arange(len(videos))\n",
        "        ax2.barh(y_pos, times, color='steelblue', alpha=0.7)\n",
        "        ax2.set_yticks(y_pos)\n",
        "        ax2.set_yticklabels(videos)\n",
        "        ax2.set_xlabel('Processing Time (seconds)', fontsize=12)\n",
        "        ax2.set_title('Processing Time by Video', fontsize=14)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('processing_time_analysis.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\nProcessing Time Statistics:\")\n",
        "        print(f\"  Mean: {np.mean(processing_times):.2f}s\")\n",
        "        print(f\"  Median: {np.median(processing_times):.2f}s\")\n",
        "        print(f\"  Min: {np.min(processing_times):.2f}s\")\n",
        "        print(f\"  Max: {np.max(processing_times):.2f}s\")\n",
        "        print(f\"  Total: {np.sum(processing_times):.2f}s\")\n",
        "        print(\"\\nProcessing time analysis saved to 'processing_time_analysis.png'\")\n",
        "    else:\n",
        "        print(\"No processing time data available.\")\n",
        "else:\n",
        "    print(\"No results to visualize. Run batch experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multiple Experiment Comparison\n",
        "\n",
        "Compare results across multiple batch experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and compare all experiments\n",
        "all_experiments = load_all_experiments()\n",
        "\n",
        "if all_experiments and len(all_experiments) > 1:\n",
        "    print(f\"Found {len(all_experiments)} experiments to compare\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    comparison_data = []\n",
        "    \n",
        "    for exp in all_experiments:\n",
        "        summary = exp['summary']\n",
        "        comparison_data.append({\n",
        "            'timestamp': exp['timestamp'],\n",
        "            'model': exp['llm_model'],\n",
        "            'total': summary['total_videos'],\n",
        "            'accuracy': summary['accuracy'],\n",
        "            'real_correct': summary.get('real_correct', 0),\n",
        "            'fake_correct': summary.get('fake_correct', 0),\n",
        "        })\n",
        "        \n",
        "        print(f\"\\nExperiment: {exp['timestamp']}\")\n",
        "        print(f\"  Model: {exp['llm_model']}\")\n",
        "        print(f\"  Accuracy: {summary['accuracy']:.1%}\")\n",
        "        print(f\"  Videos: {summary['total_videos']}\")\n",
        "    \n",
        "    # Plot comparison if we have multiple experiments\n",
        "    if len(comparison_data) >= 2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        \n",
        "        timestamps = [d['timestamp'] for d in comparison_data]\n",
        "        accuracies = [d['accuracy'] for d in comparison_data]\n",
        "        \n",
        "        ax.bar(range(len(timestamps)), accuracies, color='steelblue', alpha=0.7)\n",
        "        ax.set_xticks(range(len(timestamps)))\n",
        "        ax.set_xticklabels(timestamps, rotation=45, ha='right')\n",
        "        ax.set_ylabel('Accuracy', fontsize=12)\n",
        "        ax.set_title('Accuracy Across Experiments', fontsize=14)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        \n",
        "        for i, acc in enumerate(accuracies):\n",
        "            ax.text(i, acc + 0.02, f'{acc:.1%}', ha='center', fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('experiment_comparison.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"\\nExperiment comparison saved to 'experiment_comparison.png'\")\n",
        "else:\n",
        "    print(\"Need at least 2 experiments to compare.\")\n",
        "    print(\"Run multiple batch experiments with: python demo.py --batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final summary report\n",
        "print(\"=\"*80)\n",
        "print(\"          DEEPFAKE DETECTION SYSTEM - ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if experiment_data:\n",
        "    summary = experiment_data['summary']\n",
        "    \n",
        "    print(f\"\\n1. EXPERIMENT DETAILS\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"   Timestamp: {experiment_data['timestamp']}\")\n",
        "    print(f\"   LLM Provider: {experiment_data['llm_provider']}\")\n",
        "    print(f\"   Model: {experiment_data['llm_model']}\")\n",
        "    \n",
        "    print(f\"\\n2. DATASET\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"   Total Videos: {summary['total_videos']}\")\n",
        "    print(f\"   Real Videos: {summary['real_videos_count']}\")\n",
        "    print(f\"   Fake Videos: {summary['fake_videos_count']}\")\n",
        "    \n",
        "    print(f\"\\n3. PERFORMANCE METRICS\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"   Overall Accuracy: {summary['accuracy']:.1%}\")\n",
        "    if summary['real_videos_count'] > 0:\n",
        "        real_acc = summary['real_correct'] / summary['real_videos_count']\n",
        "        print(f\"   Real Video Accuracy: {real_acc:.1%} ({summary['real_correct']}/{summary['real_videos_count']})\")\n",
        "    if summary['fake_videos_count'] > 0:\n",
        "        fake_acc = summary['fake_correct'] / summary['fake_videos_count']\n",
        "        print(f\"   Fake Video Accuracy: {fake_acc:.1%} ({summary['fake_correct']}/{summary['fake_videos_count']})\")\n",
        "    \n",
        "    # Calculate and display additional metrics\n",
        "    if results:\n",
        "        metrics = calculate_metrics(results)\n",
        "        print(f\"   Precision: {metrics['precision']:.1%}\")\n",
        "        print(f\"   Recall: {metrics['recall']:.1%}\")\n",
        "        print(f\"   F1 Score: {metrics['f1_score']:.3f}\")\n",
        "    \n",
        "    print(f\"\\n4. GENERATED VISUALIZATIONS\")\n",
        "    print(\"-\"*40)\n",
        "    print(\"   - confusion_matrix.png\")\n",
        "    print(\"   - confidence_comparison.png\")\n",
        "    print(\"   - per_video_analysis.png\")\n",
        "    print(\"   - processing_time_analysis.png\")\n",
        "    if len(all_experiments) > 1:\n",
        "        print(\"   - experiment_comparison.png\")\n",
        "    \n",
        "    print(f\"\\n5. RECOMMENDATIONS\")\n",
        "    print(\"-\"*40)\n",
        "    if summary['accuracy'] < 0.7:\n",
        "        print(\"   - Consider adding more diverse training samples\")\n",
        "        print(\"   - Try different LLM models for better reasoning\")\n",
        "    else:\n",
        "        print(\"   - System performing well on current dataset\")\n",
        "        print(\"   - Consider testing on larger, more diverse datasets\")\n",
        "else:\n",
        "    print(\"\\nNo experiment data available.\")\n",
        "    print(\"\\nTo run a batch experiment:\")\n",
        "    print(\"  1. Place real videos in: data/samples/real/\")\n",
        "    print(\"  2. Place fake videos in: data/samples/fake/\")\n",
        "    print(\"  3. Run: python demo.py --batch\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"                    Analysis Complete!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. How to Add More Videos\n",
        "\n",
        "To run experiments with additional videos:\n",
        "\n",
        "1. **Add Real Videos:**\n",
        "   ```bash\n",
        "   cp your_real_video.mp4 data/samples/real/\n",
        "   ```\n",
        "\n",
        "2. **Add Fake Videos:**\n",
        "   ```bash\n",
        "   cp your_deepfake_video.mp4 data/samples/fake/\n",
        "   ```\n",
        "\n",
        "3. **Run Batch Experiment:**\n",
        "   ```bash\n",
        "   python demo.py --batch\n",
        "   ```\n",
        "\n",
        "4. **Re-run this notebook** to analyze the new results.\n",
        "\n",
        "The system supports: MP4, AVI, MOV, MKV, WebM formats."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
